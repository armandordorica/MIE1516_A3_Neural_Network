{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# (E6) Autoencoders\n",
    "In this exercise, you will be given an example of [autoencoders](https://en.wikipedia.org/wiki/Autoencoder). \n",
    "You should be able to replicate the results given here if you have completed (E2)-(E5) correctly.\n",
    "\n",
    "It would be best if you have a Python IDE (integrated development environment) such as [PyCharm](https://www.jetbrains.com/pycharm/) and [Anaconda](anaconda.com) is installed because they will make your life easier! If not, you may want to work on the assignment using Google Colab. In any cases, what you need to do is 1) to fill in the blanks in .py files; and 2) to import the files (e.g., layer.py, optim.py, model.py, etc) that you have completed for use. Here are some scenarios how you would go about doing the assignment: \n",
    "\n",
    "#### Without Google Colab: Python IDE + Anaconda \n",
    "If you have a Python IDE and Anaconda installed, you can do one of the following:\n",
    "- Edit .py files in the IDE. Then, simply open .ipynb file also in the IDE where you can edit and run codes. \n",
    "- Your IDE might not support running .ipynb files. However, since you have installed Anaconda, you can just open this notebook using Jupyter Notebook.\n",
    "\n",
    "In both of these cases, you can simply import .py files in this .ipynb file:\n",
    "```python\n",
    "from model import NeuralNetwork\n",
    "```\n",
    " \n",
    "#### With Google Colab\n",
    "- Google Colab has an embedded code editor. So, you could simply upload all .py files to Google Colab and edit the files there. Once you upload the files, double click a file that you want to edit. Please **make sure that you download up-to-date files frequently**, otherwise Google Colab might accidentally restart and all your files might be gone.\n",
    "- If you feel like the above way is cumbersome, you could instead use any online Python editors for completing .py files (e.g., see [repl.it](https://repl.it/languages/python3)). Also, it's not impossible that you edit the files using any text editors, but they don't show you essential Python grammar information, so you'll be prone to make mistakes in that case. Once you are done editing, you can either upload the files to Colab or follow the instruction below. \n",
    " \n",
    "- If you have *git clone*d the assignment repository to a directory in your Google Drive (or you have the files stored in the Drive anyway), you can do the following:\n",
    "```jupyterpython\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')          # this will direct you to a link where you can get an authorization key\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/your-directory-where-the-python-files-exist')\n",
    "```\n",
    "Then, you are good to go. When you change a .py file, make sure it is synced to the drive, then you need to re-run the above lines to get access to the latest version of the file. Note that you should give correct path to *sys.path.append* method.\n",
    "\n",
    "Now, let's get started!\n",
    "## Autoencoder\n",
    "### Input and Target\n",
    "An autoencoder learns the latent embeddings of inputs in an unsupervised way. This is because we do not need to have specific target values associated with the inputs; however, the input data themselves will act as the targets. \n",
    "\n",
    "To see it more concretely, let's look at below code which prepares the data for learning an autoencoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from model import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def generate_data(num=8):\n",
    "    \"\"\" Generate 'num' number of one-hot encoded integers. \"\"\" \n",
    "    x_train = np.eye(num)[np.arange(num)]                       # This is a simple way to one-hot encode integers\n",
    "    \n",
    "    # Repeat x_train multiple times for training\n",
    "    x_train = np.repeat(x_train, 100, axis=0)\n",
    "    \n",
    "    # The target is x_train itself!\n",
    "    x_target = x_train.copy()\n",
    "    return x_train, x_target    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Clearly, *x_target* is the same as *x_train*. So, what we want to do is to encode 8-bit inputs using 3 hidden nodes, which in turn will be decoded back to the original 8-bit value by the decoder. Learning an autoencoder, therefore, means that we train both the encoder weight and the decoder weight. In our example, since we have 3 hidden nodes in a single layer, the encoder weight has *[8, 3]* shape, whereas the decoder weight has *[3, 8]* shape. \n",
    "\n",
    "### Training an Autoencoder\n",
    "Now, let us train an autoencoder with the sigmoid activation function and the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from model import NeuralNetwork\n",
    "from layer import FCLayer\n",
    "from activation import Activation\n",
    "from utils import *\n",
    "from loss import CrossEntropyLoss\n",
    "from optim import SGD, Adam, RMSProp\n",
    "# Load data\n",
    "num = 8\n",
    "np.random.seed(10)\n",
    "x_train, x_target = generate_data(num=num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define a model and add fully-connected and activation layers.\n",
    "nn = NeuralNetwork()\n",
    "nn.add(FCLayer(x_train.shape[1], 3, initialization='xavier', uniform=False))\n",
    "nn.add(Activation(sigmoid, sigmoid_prime))\n",
    "nn.add(FCLayer(3, x_train.shape[1], initialization='xavier', uniform=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<layer.FCLayer at 0x1329a4d68>,\n",
       " <activation.Activation at 0x1329a4dd8>,\n",
       " <layer.FCLayer at 0x1329a4f28>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Assigning the Cross Entropy Loss functions to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define loss: note that CrossEntropyLoss is using the softmax output internally\n",
    "loss = CrossEntropyLoss()\n",
    "nn.set_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set up hyperparameters\n",
    "lr = 0.001\n",
    "epochs = 2000\n",
    "freq = epochs // 10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.parameters()[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3)\n",
      "(1, 3)\n",
      "(3, 8)\n",
      "(1, 8)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(nn.parameters())):\n",
    "    print(nn.parameters()[i].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define optimizer and associate it with the model\n",
    "optimizer = Adam(nn.parameters(), lr=lr)\n",
    "nn.set_optimizer(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2000\tloss=0.03323\tTest loss: 2.11283\n",
      "Epoch 201/2000\tloss=0.01168\tTest loss: 0.74589\n",
      "Epoch 401/2000\tloss=0.00379\tTest loss: 0.12764\n",
      "Epoch 601/2000\tloss=0.00123\tTest loss: 0.05228\n",
      "Epoch 801/2000\tloss=0.00042\tTest loss: 0.01898\n",
      "Epoch 1001/2000\tloss=0.00015\tTest loss: 0.00911\n",
      "Epoch 1201/2000\tloss=0.00006\tTest loss: 0.00410\n",
      "Epoch 1401/2000\tloss=0.00002\tTest loss: 0.00135\n",
      "Epoch 1601/2000\tloss=0.00001\tTest loss: 0.00054\n",
      "Epoch 1801/2000\tloss=0.00000\tTest loss: 0.00015\n",
      "Training finished!\n",
      "Print prediction results:\n",
      "\tInput: [1. 0. 0. 0. 0. 0. 0. 0.]\tOutput: [[1.00e+00 3.17e-15 4.70e-05 1.37e-11 4.63e-06 1.75e-08 2.07e-11 2.43e-05]]\n",
      "\tInput: [0. 1. 0. 0. 0. 0. 0. 0.]\tOutput: [[3.25e-16 1.00e+00 2.21e-10 1.71e-05 1.10e-09 5.11e-05 1.66e-05 1.16e-10]]\n",
      "\tInput: [0. 0. 1. 0. 0. 0. 0. 0.]\tOutput: [[3.05e-05 1.81e-10 1.00e+00 3.39e-16 9.14e-10 4.94e-05 2.47e-05 1.65e-10]]\n",
      "\tInput: [0. 0. 0. 1. 0. 0. 0. 0.]\tOutput: [[1.51e-11 2.54e-05 1.48e-14 1.00e+00 8.08e-06 2.58e-08 2.03e-11 2.42e-05]]\n",
      "\tInput: [0. 0. 0. 0. 1. 0. 0. 0.]\tOutput: [[2.28e-05 1.73e-08 8.69e-08 1.86e-05 1.00e+00 2.10e-10 3.26e-05 1.44e-08]]\n",
      "\tInput: [0. 0. 0. 0. 0. 1. 0. 0.]\tOutput: [[3.36e-09 4.26e-05 2.82e-05 2.93e-09 1.19e-12 1.00e+00 2.47e-09 4.60e-05]]\n",
      "\tInput: [0. 0. 0. 0. 0. 0. 1. 0.]\tOutput: [[1.84e-11 2.56e-05 4.85e-05 1.20e-11 5.11e-06 1.57e-08 1.00e+00 2.59e-15]]\n",
      "\tInput: [0. 0. 0. 0. 0. 0. 0. 1.]\tOutput: [[1.63e-05 1.12e-10 1.96e-10 1.78e-05 9.08e-10 5.22e-05 3.12e-16 1.00e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Training begins\n",
    "inds = list(range(x_train.shape[0]))\n",
    "N = x_train.shape[0]\n",
    "\n",
    "loss_hist = []\n",
    "total_pred_hist = []\n",
    "total_loss_hist = []\n",
    "for epoch in range(epochs):\n",
    "    inds = np.random.permutation(inds)\n",
    "    x_train = x_train[inds]\n",
    "    x_target = x_target[inds]\n",
    "    \n",
    "    loss = 0\n",
    "    for b in range(0, N, batch_size):\n",
    "        #0, 800, 64 \n",
    "        # get the mini-batch\n",
    "#         print(\"b:{}\".format(b))\n",
    "#         print(\"b+batch_size:{}\".format(b+batch_size))\n",
    "        x_batch = x_train[b: b+batch_size]\n",
    "#         print(\"x_batch:{}\".format(x_batch))\n",
    "        x_target_batch = x_target[b: b+batch_size]\n",
    "        \n",
    "        # feed forward\n",
    "#         print(x_batch)\n",
    "        pred = nn.predict(x_batch)\n",
    "        total_pred_hist.append(pred)\n",
    "        #pred = result from forward pass, i.e. H^{l-1}\n",
    "        \n",
    "        # Error\n",
    "        loss += nn.loss(pred, x_target_batch)/N\n",
    "        total_loss_hist.append(loss)\n",
    "        #this is the delta from the loss function\n",
    "        \n",
    "        # Back propagation of error\n",
    "        nn.backward(pred, x_target_batch)\n",
    "        \n",
    "        # Update parameters\n",
    "        nn.optimizer.step()\n",
    "    \n",
    "    # Record loss per epoch\n",
    "    loss_hist.append(loss)\n",
    "    \n",
    "    if epoch % freq == 0:\n",
    "        print()\n",
    "        print(\"Epoch %d/%d\\tloss=%.5f\" % (epoch + 1, epochs, loss), end='\\t', flush=True)\n",
    "        \n",
    "        # Test with the training data\n",
    "        pred = nn.predict(x_train, mode=False)\n",
    "        l = nn.loss(pred, x_target)\n",
    "        print(\"Test loss: {:.5f}\".format(l), end='')\n",
    "\n",
    "print(\"\\nTraining finished!\")\n",
    "print(\"Print prediction results:\")\n",
    "x_test = np.eye(num)[np.arange(num)]                        # Test data (one-hot encoded)\n",
    "np.set_printoptions(2)\n",
    "for x in x_test:\n",
    "    print(\"\\tInput: {}\\tOutput: {}\".format(x, softmax(nn.predict(x[None, :], mode=False))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights of the first FCLayer:\n",
      " [[-8.46  6.3  -7.67]\n",
      " [ 8.01 -9.22  7.43]\n",
      " [ 8.06  5.35 -8.11]\n",
      " [-8.16 -8.69  7.67]\n",
      " [-8.4  -8.28 -7.72]\n",
      " [ 8.82  6.73  7.52]\n",
      " [ 7.81 -8.52 -7.65]\n",
      " [-8.64  6.8   7.64]]\n",
      "Shape: (8, 3)\n"
     ]
    }
   ],
   "source": [
    "print('weights of the first FCLayer:\\n', nn._parameters[0].value)\n",
    "print('Shape:', nn._parameters[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If you look at the output values of the network, clearly we have successfully trained the autoencoder to encode-decode 8-bit integers!\n",
    "\n",
    "## (E7) Your Turn:  Explain the autoencoder\n",
    "Given the trained model that can encode the 0-7 integers, explain how the NN model learned to encode/compress the numbers. Rather than just stating your reasoning in words, do explore the model closely to see what it has learned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "loss(x, y) = - \\frac{1}{C} * \\sum_i y[i] * \\log((1 + \\exp(-x[i]))^{-1})\n",
    "                         + (1-y[i]) * \\log\\left(\\frac{\\exp(-x[i])}{(1 + \\exp(-x[i]))}\\right)\n",
    "                         $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole point of autoencoders is to achieve dimensionality reduction by removing the noise in the signal and being able to recreate the same input X with less factors. Here we can see this effect taking place since our weights and biases that we have calculated can recreate the whole input set with high accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_hist = np.array(loss_hist).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(loss_hist) # Plot list. x-values assumed to be [0, 1, 2, 3]\n",
    "plt.title(\"Loss history\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data to this layer:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Shape of each weight vector for FCN layer 1:(8, 3)\n",
      "Weight vector in this layer:\n",
      "[[-8.46  6.3  -7.67]\n",
      " [ 8.01 -9.22  7.43]\n",
      " [ 8.06  5.35 -8.11]\n",
      " [-8.16 -8.69  7.67]\n",
      " [-8.4  -8.28 -7.72]\n",
      " [ 8.82  6.73  7.52]\n",
      " [ 7.81 -8.52 -7.65]\n",
      " [-8.64  6.8   7.64]]\n",
      ".............\n",
      "Shape of each weight vector for FCN layer 1:(1, 3)\n",
      "Bias vector in this layer:[[-0.05  1.07 -0.05]]\n",
      "Output of first fully connected layer:\n",
      "[[-8.69  7.87  7.58]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input data to this layer:\\n{}\".format(nn.layers[0].input_data))\n",
    "print(\"Shape of each weight vector for FCN layer 1:{}\".format(nn.layers[0].all_weights[0].shape))\n",
    "print(\"Weight vector in this layer:\\n{}\".format(nn.layers[0].all_weights[0]))\n",
    "print(\".............\")\n",
    "print(\"Shape of each weight vector for FCN layer 1:{}\".format(nn.layers[0].bias.value.shape))\n",
    "print(\"Bias vector in this layer:{}\".format(nn.layers[0].bias.value))\n",
    "print(\"Output of first fully connected layer:\\n{}\".format(nn.layers[0].output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data to this layer:[[-8.69  7.87  7.58]]\n",
      "Activation function for this layer:<function sigmoid at 0x1234aea60>\n",
      "Diff loss for this layer:<function sigmoid_prime at 0x1234aeae8>\n",
      "Output data to this layer:[[1.68e-04 1.00e+00 9.99e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input data to this layer:{}\".format(nn.layers[1].input_data))\n",
    "print(\"Activation function for this layer:{}\".format(nn.layers[1].act))\n",
    "print(\"Diff loss for this layer:{}\".format(nn.layers[1].act_prime))\n",
    "print(\"Output data to this layer:{}\".format(nn.layers[1].output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data to this layer:\n",
      "[[1.68e-04 1.00e+00 9.99e-01]]\n",
      "Shape of each weight vector for FCN layer 1:(3, 8)\n",
      "Weight vector in this layer:\n",
      "[[-13.64   7.7    6.73 -13.87 -11.8    4.71  10.74 -15.14]\n",
      " [ 10.88 -15.36   6.48 -13.97 -12.13   4.6  -14.11   7.61]\n",
      " [-13.7    7.83 -15.07  11.43 -11.21   5.35 -13.77   7.97]]\n",
      ".............\n",
      "Shape of each weight vector for FCN layer 1:(1, 8)\n",
      "Bias vector in this layer:[[ 2.   -5.18 -3.57  1.8  12.7  -9.6   2.37 -5.38]]\n",
      "Output of first fully connected layer:\n",
      "[[ -0.83 -12.71 -12.16  -0.74 -10.62   0.34 -25.51  10.2 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input data to this layer:\\n{}\".format(nn.layers[2].input_data))\n",
    "print(\"Shape of each weight vector for FCN layer 1:{}\".format(nn.layers[2].all_weights[0].shape))\n",
    "print(\"Weight vector in this layer:\\n{}\".format(nn.layers[2].all_weights[0]))\n",
    "print(\".............\")\n",
    "print(\"Shape of each weight vector for FCN layer 1:{}\".format(nn.layers[2].bias.value.shape))\n",
    "print(\"Bias vector in this layer:{}\".format(nn.layers[2].bias.value))\n",
    "print(\"Output of first fully connected layer:\\n{}\".format(nn.layers[2].output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(U):\n",
    "    # A numerically-stable implementation of the softmax function\n",
    "    return np.exp(array)/sum(np.exp(array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73 0.5  0.5  0.5  0.5  0.5  0.5  0.5 ]\n",
      "[0.5  0.73 0.5  0.5  0.5  0.5  0.5  0.5 ]\n",
      "[0.5  0.5  0.73 0.5  0.5  0.5  0.5  0.5 ]\n",
      "[0.5  0.5  0.5  0.73 0.5  0.5  0.5  0.5 ]\n",
      "[0.5  0.5  0.5  0.5  0.73 0.5  0.5  0.5 ]\n",
      "[0.5  0.5  0.5  0.5  0.5  0.73 0.5  0.5 ]\n",
      "[0.5  0.5  0.5  0.5  0.5  0.5  0.73 0.5 ]\n",
      "[0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.73]\n"
     ]
    }
   ],
   "source": [
    "for i in x_test:\n",
    "    print(nn.layers[1].forward(i, mode=False))\n",
    "#     print(nn.layers[1].forward(i, mode=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "array = nn.layers[1].forward(i, mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.15])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-c36b532628bd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-c36b532628bd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    softmax(x) = /sum(np.exp(x))\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "softmax(x) = /sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.exp(array)/sum(np.exp(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
