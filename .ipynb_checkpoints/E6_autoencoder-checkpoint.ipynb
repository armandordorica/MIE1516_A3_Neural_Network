{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# (E6) Autoencoders\n",
    "In this exercise, you will be given an example of [autoencoders](https://en.wikipedia.org/wiki/Autoencoder). \n",
    "You should be able to replicate the results given here if you have completed (E2)-(E5) correctly.\n",
    "\n",
    "It would be best if you have a Python IDE (integrated development environment) such as [PyCharm](https://www.jetbrains.com/pycharm/) and [Anaconda](anaconda.com) is installed because they will make your life easier! If not, you may want to work on the assignment using Google Colab. In any cases, what you need to do is 1) to fill in the blanks in .py files; and 2) to import the files (e.g., layer.py, optim.py, model.py, etc) that you have completed for use. Here are some scenarios how you would go about doing the assignment: \n",
    "\n",
    "#### Without Google Colab: Python IDE + Anaconda \n",
    "If you have a Python IDE and Anaconda installed, you can do one of the following:\n",
    "- Edit .py files in the IDE. Then, simply open .ipynb file also in the IDE where you can edit and run codes. \n",
    "- Your IDE might not support running .ipynb files. However, since you have installed Anaconda, you can just open this notebook using Jupyter Notebook.\n",
    "\n",
    "In both of these cases, you can simply import .py files in this .ipynb file:\n",
    "```python\n",
    "from model import NeuralNetwork\n",
    "```\n",
    " \n",
    "#### With Google Colab\n",
    "- Google Colab has an embedded code editor. So, you could simply upload all .py files to Google Colab and edit the files there. Once you upload the files, double click a file that you want to edit. Please **make sure that you download up-to-date files frequently**, otherwise Google Colab might accidentally restart and all your files might be gone.\n",
    "- If you feel like the above way is cumbersome, you could instead use any online Python editors for completing .py files (e.g., see [repl.it](https://repl.it/languages/python3)). Also, it's not impossible that you edit the files using any text editors, but they don't show you essential Python grammar information, so you'll be prone to make mistakes in that case. Once you are done editing, you can either upload the files to Colab or follow the instruction below. \n",
    " \n",
    "- If you have *git clone*d the assignment repository to a directory in your Google Drive (or you have the files stored in the Drive anyway), you can do the following:\n",
    "```jupyterpython\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')          # this will direct you to a link where you can get an authorization key\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/your-directory-where-the-python-files-exist')\n",
    "```\n",
    "Then, you are good to go. When you change a .py file, make sure it is synced to the drive, then you need to re-run the above lines to get access to the latest version of the file. Note that you should give correct path to *sys.path.append* method.\n",
    "\n",
    "Now, let's get started!\n",
    "## Autoencoder\n",
    "### Input and Target\n",
    "An autoencoder learns the latent embeddings of inputs in an unsupervised way. This is because we do not need to have specific target values associated with the inputs; however, the input data themselves will act as the targets. \n",
    "\n",
    "To see it more concretely, let's look at below code which prepares the data for learning an autoencoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from model import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def generate_data(num=8):\n",
    "    \"\"\" Generate 'num' number of one-hot encoded integers. \"\"\" \n",
    "    x_train = np.eye(num)[np.arange(num)]                       # This is a simple way to one-hot encode integers\n",
    "    \n",
    "    # Repeat x_train multiple times for training\n",
    "    x_train = np.repeat(x_train, 100, axis=0)\n",
    "    \n",
    "    # The target is x_train itself!\n",
    "    x_target = x_train.copy()\n",
    "    return x_train, x_target    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Clearly, *x_target* is the same as *x_train*. So, what we want to do is to encode 8-bit inputs using 3 hidden nodes, which in turn will be decoded back to the original 8-bit value by the decoder. Learning an autoencoder, therefore, means that we train both the encoder weight and the decoder weight. In our example, since we have 3 hidden nodes in a single layer, the encoder weight has *[8, 3]* shape, whereas the decoder weight has *[3, 8]* shape. \n",
    "\n",
    "### Training an Autoencoder\n",
    "Now, let us train an autoencoder with the sigmoid activation function and the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from model import NeuralNetwork\n",
    "from layer import FCLayer\n",
    "from activation import Activation\n",
    "from utils import *\n",
    "from loss import CrossEntropyLoss\n",
    "from optim import SGD, Adam, RMSProp\n",
    "# Load data\n",
    "num = 8\n",
    "np.random.seed(10)\n",
    "x_train, x_target = generate_data(num=num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define a model and add fully-connected and activation layers.\n",
    "nn = NeuralNetwork()\n",
    "nn.add(FCLayer(x_train.shape[1], 3, initialization='xavier', uniform=False))\n",
    "nn.add(Activation(sigmoid, sigmoid_prime))\n",
    "nn.add(FCLayer(3, x_train.shape[1], initialization='xavier', uniform=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<layer.FCLayer at 0x12593f978>,\n",
       " <activation.Activation at 0x12593f9e8>,\n",
       " <layer.FCLayer at 0x12593fac8>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Assigning the Cross Entropy Loss functions to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define loss: note that CrossEntropyLoss is using the softmax output internally\n",
    "loss = CrossEntropyLoss()\n",
    "nn.set_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set up hyperparameters\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "freq = epochs // 10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.parameters()[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3)\n",
      "(1, 3)\n",
      "(3, 8)\n",
      "(1, 8)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(nn.parameters())):\n",
    "    print(nn.parameters()[i].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define optimizer and associate it with the model\n",
    "optimizer = Adam(nn.parameters(), lr=lr)\n",
    "nn.set_optimizer(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:17.012284697811733\n",
      "loss:17.025722381630224\n",
      "loss:17.040211069076747\n",
      "loss:17.045639830005275\n",
      "loss:17.04049323911932\n",
      "loss:17.045497038424994\n",
      "loss:17.038706562398943\n",
      "loss:17.040458330734317\n",
      "loss:17.05661757723084\n",
      "loss:17.07049025673983\n",
      "loss:17.075437447779166\n",
      "loss:17.076932306958767\n",
      "loss:17.088827853052436\n",
      "\n",
      "Epoch 1/10\tloss=221.65732\tloss:17.074350833208328\n",
      "Test loss: 17.07435loss:17.065354047362618\n",
      "loss:17.074928823662\n",
      "loss:17.07697847495637\n",
      "loss:17.090620449508307\n",
      "loss:17.085634802623566\n",
      "loss:17.10159987583182\n",
      "loss:17.092164149464928\n",
      "loss:17.111104286473825\n",
      "loss:17.109469444735495\n",
      "loss:17.117163397843377\n",
      "loss:17.111190766118813\n",
      "loss:17.092874711198952\n",
      "loss:17.101628080137218\n",
      "\n",
      "Epoch 2/10\tloss=222.23071\tloss:17.12048039408945\n",
      "Test loss: 17.12048loss:17.12516757798049\n",
      "loss:17.117202671050975\n",
      "loss:17.117832003532094\n",
      "loss:17.11869216526406\n",
      "loss:17.133506691277514\n",
      "loss:17.141539937482776\n",
      "loss:17.127454538628786\n",
      "loss:17.132588383859925\n",
      "loss:17.16473966867975\n",
      "loss:17.155871752471835\n",
      "loss:17.152103719504325\n",
      "loss:17.194475597555186\n",
      "loss:17.12991622567919\n",
      "\n",
      "Epoch 3/10\tloss=222.81109\tloss:17.16671128303956\n",
      "Test loss: 17.16671loss:17.18979413348993\n",
      "loss:17.164239768838712\n",
      "loss:17.13251874616047\n",
      "loss:17.18534504935056\n",
      "loss:17.178327007841034\n",
      "loss:17.18268011243846\n",
      "loss:17.182734997976983\n",
      "loss:17.16912317567038\n",
      "loss:17.199792413965984\n",
      "loss:17.23373826306492\n",
      "loss:17.21342085377792\n",
      "loss:17.221741174843444\n",
      "loss:17.244595001194455\n",
      "\n",
      "Epoch 4/10\tloss=223.49805\tloss:17.22043276945348\n",
      "Test loss: 17.22043loss:17.21633352842206\n",
      "loss:17.212468127167753\n",
      "loss:17.245735597985632\n",
      "loss:17.19036322857026\n",
      "loss:17.223043267745055\n",
      "loss:17.246587595410134\n",
      "loss:17.27599184499158\n",
      "loss:17.248818554611486\n",
      "loss:17.247121111563956\n",
      "loss:17.253071316370203\n",
      "loss:17.2631840268673\n",
      "loss:17.321584072476604\n",
      "loss:17.28823682671976\n",
      "\n",
      "Epoch 5/10\tloss=224.23254\tloss:17.281345238669914\n",
      "Test loss: 17.28135loss:17.27092565528478\n",
      "loss:17.294322214281593\n",
      "loss:17.314504141275496\n",
      "loss:17.26059236710911\n",
      "loss:17.30910392963814\n",
      "loss:17.320836048597368\n",
      "loss:17.301271714608532\n",
      "loss:17.33722475000156\n",
      "loss:17.302323835941866\n",
      "loss:17.314938192728732\n",
      "loss:17.336864518587383\n",
      "loss:17.332994516272194\n",
      "loss:17.377233014058923\n",
      "\n",
      "Epoch 6/10\tloss=225.07313\tloss:17.34859941992063\n",
      "Test loss: 17.34860loss:17.346909427600057\n",
      "loss:17.334915424884336\n",
      "loss:17.32753137620587\n",
      "loss:17.327891033882583\n",
      "loss:17.41196335961712\n",
      "loss:17.364958262593113\n",
      "loss:17.354663962460435\n",
      "loss:17.374181175798856\n",
      "loss:17.4382670293478\n",
      "loss:17.416005620808512\n",
      "loss:17.40283031905619\n",
      "loss:17.46980771132208\n",
      "loss:17.34711802495449\n",
      "\n",
      "Epoch 7/10\tloss=225.91704\tloss:17.420916402765936\n",
      "Test loss: 17.42092loss:17.4311803404726\n",
      "loss:17.41769232507728\n",
      "loss:17.406117387509855\n",
      "loss:17.407161260946918\n",
      "loss:17.47212009856674\n",
      "loss:17.448152345977626\n",
      "loss:17.490359052630644\n",
      "loss:17.473440186154313\n",
      "loss:17.386913811489258\n",
      "loss:17.49736049084442\n",
      "loss:17.507001882125866\n",
      "loss:17.55834006451098\n",
      "loss:17.454816717919982\n",
      "\n",
      "Epoch 8/10\tloss=226.95066\tloss:17.506104361889516\n",
      "Test loss: 17.50610loss:17.59291095016392\n",
      "loss:17.525998227709326\n",
      "loss:17.514870214530127\n",
      "loss:17.47920384350185\n",
      "loss:17.521356211027975\n",
      "loss:17.50467520138018\n",
      "loss:17.565404118879812\n",
      "loss:17.55365096435891\n",
      "loss:17.55199773387689\n",
      "loss:17.614531121964106\n",
      "loss:17.50970591390484\n",
      "loss:17.56899697577933\n",
      "loss:17.611498439335328\n",
      "\n",
      "Epoch 9/10\tloss=228.11480\tloss:17.59671745352788\n",
      "Test loss: 17.59672loss:17.662067444013925\n",
      "loss:17.59771146568442\n",
      "loss:17.672440090358926\n",
      "loss:17.599220127144992\n",
      "loss:17.562073653381425\n",
      "loss:17.653770422128197\n",
      "loss:17.654087609907037\n",
      "loss:17.624479927965236\n",
      "loss:17.660924309855247\n",
      "loss:17.61404711392838\n",
      "loss:17.676927309810196\n",
      "loss:17.652618323253478\n",
      "loss:17.720200614971723\n",
      "\n",
      "Epoch 10/10\tloss=229.35057\tloss:17.697849700943287\n",
      "Test loss: 17.69785\n",
      "Training finished!\n",
      "Print prediction results:\n",
      "\tInput: [1. 0. 0. 0. 0. 0. 0. 0.]\tOutput: [[0.14 0.13 0.07 0.14 0.08 0.18 0.08 0.18]]\n",
      "\tInput: [0. 1. 0. 0. 0. 0. 0. 0.]\tOutput: [[0.17 0.12 0.06 0.13 0.07 0.19 0.07 0.18]]\n",
      "\tInput: [0. 0. 1. 0. 0. 0. 0. 0.]\tOutput: [[0.19 0.12 0.05 0.13 0.06 0.21 0.05 0.19]]\n",
      "\tInput: [0. 0. 0. 1. 0. 0. 0. 0.]\tOutput: [[0.18 0.13 0.04 0.14 0.05 0.21 0.04 0.2 ]]\n",
      "\tInput: [0. 0. 0. 0. 1. 0. 0. 0.]\tOutput: [[0.18 0.13 0.04 0.14 0.05 0.21 0.05 0.2 ]]\n",
      "\tInput: [0. 0. 0. 0. 0. 1. 0. 0.]\tOutput: [[0.18 0.12 0.05 0.13 0.06 0.21 0.06 0.19]]\n",
      "\tInput: [0. 0. 0. 0. 0. 0. 1. 0.]\tOutput: [[0.18 0.12 0.05 0.13 0.06 0.21 0.06 0.19]]\n",
      "\tInput: [0. 0. 0. 0. 0. 0. 0. 1.]\tOutput: [[0.17 0.13 0.05 0.14 0.06 0.2  0.06 0.19]]\n"
     ]
    }
   ],
   "source": [
    "# Training begins\n",
    "inds = list(range(x_train.shape[0]))\n",
    "N = x_train.shape[0]\n",
    "\n",
    "loss_hist = []\n",
    "for epoch in range(epochs):\n",
    "    inds = np.random.permutation(inds)\n",
    "    x_train = x_train[inds]\n",
    "    x_target = x_target[inds]\n",
    "    \n",
    "    loss = 0\n",
    "    for b in range(0, N, batch_size):\n",
    "        #0, 800, 64 \n",
    "        # get the mini-batch\n",
    "#         print(\"b:{}\".format(b))\n",
    "#         print(\"b+batch_size:{}\".format(b+batch_size))\n",
    "        x_batch = x_train[b: b+batch_size]\n",
    "#         print(\"x_batch:{}\".format(x_batch))\n",
    "        x_target_batch = x_target[b: b+batch_size]\n",
    "        \n",
    "        # feed forward\n",
    "#         print(x_batch)\n",
    "        pred = nn.predict(x_batch)\n",
    "        #pred = result from forward pass, i.e. H^{l-1}\n",
    "        \n",
    "        # Error\n",
    "        loss += nn.loss(pred, x_target_batch)/N\n",
    "        #this is the delta from the loss function\n",
    "        \n",
    "        # Back propagation of error\n",
    "        nn.backward(pred, x_target_batch)\n",
    "        \n",
    "        # Update parameters\n",
    "        nn.optimizer.step()\n",
    "    \n",
    "    # Record loss per epoch\n",
    "    loss_hist.append(loss)\n",
    "    \n",
    "    if epoch % freq == 0:\n",
    "        print()\n",
    "        print(\"Epoch %d/%d\\tloss=%.5f\" % (epoch + 1, epochs, loss), end='\\t', flush=True)\n",
    "        \n",
    "        # Test with the training data\n",
    "        pred = nn.predict(x_train, mode=False)\n",
    "        l = nn.loss(pred, x_target)\n",
    "        print(\"Test loss: {:.5f}\".format(l), end='')\n",
    "\n",
    "print(\"\\nTraining finished!\")\n",
    "print(\"Print prediction results:\")\n",
    "x_test = np.eye(num)[np.arange(num)]                        # Test data (one-hot encoded)\n",
    "np.set_printoptions(2)\n",
    "for x in x_test:\n",
    "    print(\"\\tInput: {}\\tOutput: {}\".format(x, softmax(nn.predict(x[None, :], mode=False))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If you look at the output values of the network, clearly we have successfully trained the autoencoder to encode-decode 8-bit integers!\n",
    "\n",
    "## (E7) Your Turn:  Explain the autoencoder\n",
    "Given the trained model that can encode the 0-7 integers, explain how the NN model learned to encode/compress the numbers. Rather than just stating your reasoning in words, do explore the model closely to see what it has learned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "loss(x, y) = - \\frac{1}{C} * \\sum_i y[i] * \\log((1 + \\exp(-x[i]))^{-1})\n",
    "                         + (1-y[i]) * \\log\\left(\\frac{\\exp(-x[i])}{(1 + \\exp(-x[i]))}\\right)\n",
    "                         $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29736802"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.34922997 +0.13353835-0.54969662+0.20205826-0.36256657+0.5011707-0.4363535+0.45998743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(U):\n",
    "    # A numerically-stable implementation of the softmax function\n",
    "    exp = np.exp(U - np.max(U, axis=1, keepdims=True))\n",
    "    return exp / exp.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(softmax(pred)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logaddexp(0, pred).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (8,32) and (800,8) not aligned: 32 (dim 1) != 800 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2fda3e91ff6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx_target_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogaddexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (8,32) and (800,8) not aligned: 32 (dim 1) != 800 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.dot((1-x_target_batch).T, np.logaddexp(0, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-np.sum(np.dot(target.T, np.logaddexp(0, -pred)) + np.dot((1-target).T, np.logaddexp(0, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
